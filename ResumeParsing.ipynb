{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQKMmdCpOy3-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Optional\n",
        "import pdfplumber\n",
        "import docx\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pydantic Schema"
      ],
      "metadata": {
        "id": "JKe2L6fNO49D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ContactInfo(BaseModel):\n",
        "    phone: Optional[str] = None\n",
        "    email: Optional[str] = None\n",
        "\n",
        "class ExperienceItem(BaseModel):\n",
        "    title: Optional[str] = None\n",
        "    organization: Optional[str] = None\n",
        "    duration: Optional[str] = None\n",
        "    description: Optional[str] = None\n",
        "\n",
        "class ProjectItem(BaseModel):\n",
        "    name: Optional[str] = None\n",
        "    description: Optional[str] = None\n",
        "    tech_stack: Optional[str] = None\n",
        "\n",
        "class SkillsSummary(BaseModel):\n",
        "    languages: List[str] = []\n",
        "    frameworks_libraries: List[str] = []\n",
        "    tools_platforms: List[str] = []\n",
        "\n",
        "class ExtraCurricularItem(BaseModel):\n",
        "    role: Optional[str] = None\n",
        "    organization: Optional[str] = None\n",
        "    duration: Optional[str] = None\n",
        "    description: Optional[str] = None\n",
        "\n",
        "class ResumeSchema(BaseModel):\n",
        "    name: Optional[str] = None\n",
        "    contact_info: ContactInfo = ContactInfo()\n",
        "    github_link: Optional[str] = None\n",
        "    linkedin: Optional[str] = None\n",
        "    qualification: Optional[str] = None\n",
        "    university: Optional[str] = None\n",
        "    experience: List[ExperienceItem] = []\n",
        "    projects: List[ProjectItem] = []\n",
        "    coursework_keywords: List[str] = []\n",
        "    skills_summary: SkillsSummary = SkillsSummary()\n",
        "    extracurricular: List[ExtraCurricularItem] = []"
      ],
      "metadata": {
        "id": "y2LZzSNmO4lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers to extract text from PDF / DOCX"
      ],
      "metadata": {
        "id": "SSzxaaLNPEQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(path: str) -> str:\n",
        "    text_parts = []\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text_parts.append(page_text)\n",
        "    return \"\\n\".join(text_parts)\n",
        "\n",
        "def extract_text_from_docx(path: str) -> str:\n",
        "    doc = docx.Document(path)\n",
        "    paragraphs = [p.text for p in doc.paragraphs if p.text and p.text.strip()]\n",
        "    return \"\\n\".join(paragraphs)\n",
        "\n",
        "def load_document_text(path: str) -> str:\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext == \".pdf\":\n",
        "        return extract_text_from_pdf(path)\n",
        "    elif ext in (\".docx\", \".doc\"):\n",
        "        return extract_text_from_docx(path)\n",
        "    else:\n",
        "        # fallback for plain text files\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()"
      ],
      "metadata": {
        "id": "8Jyc5LKOPFpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build RAG components, index and retriever"
      ],
      "metadata": {
        "id": "q-UkdO3VPLpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_retriever_from_text(text: str, embedding_model=None, persist_path=None):\n",
        "    # Split into chunks\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "    docs = splitter.create_documents([text])\n",
        "\n",
        "    # Embeddings\n",
        "    embeddings = embedding_model or OpenAIEmbeddings()\n",
        "    # Build FAISS index\n",
        "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "    return vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "WRgIcgAUPMjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt + parser for the schema"
      ],
      "metadata": {
        "id": "srb-jJCePTOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_parser_and_prompt():\n",
        "    parser = PydanticOutputParser(pydantic_object=ResumeSchema)\n",
        "\n",
        "    instruction = \"\"\"You are given text extracted from a resume/CV. Extract the information and return a JSON that EXACTLY conforms to the provided Pydantic schema.\n",
        "- Only return JSON and nothing else.\n",
        "- If a field is not present, set it to null or an empty list/object as appropriate.\n",
        "- Parse dates/durations as they appear (do not normalize unless obvious).\n",
        "- Aggregate skills/frameworks/tools as lists.\n",
        "\n",
        "Schema: {schema}\n",
        "\n",
        "Resume text:\n",
        "{resume_text}\n",
        "\n",
        "Return the JSON now.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"schema\", \"resume_text\"],\n",
        "        template=instruction,\n",
        "    )\n",
        "\n",
        "    return parser, prompt\n"
      ],
      "metadata": {
        "id": "1A_6QwdVPUQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parse resume -> JSON using RAG + LLM"
      ],
      "metadata": {
        "id": "elPpgSPQPb7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_resume_to_json(path: str, openai_api_key: Optional[str] = None):\n",
        "    if openai_api_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "    # 1) Load text\n",
        "    raw_text = load_document_text(path)\n",
        "    if not raw_text.strip():\n",
        "        raise ValueError(\"No text extracted from file.\")\n",
        "\n",
        "    # 2) Build retriever (RAG)\n",
        "    retriever = build_retriever_from_text(raw_text)\n",
        "\n",
        "    # 3) LLM (chat model)\n",
        "    llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\" )\n",
        "    # 4) Parser + prompt\n",
        "    parser, prompt_template = build_parser_and_prompt()\n",
        "\n",
        "    # 5) Build retrieval QA chain - we will pass the resume text trimmed into the prompt.\n",
        "    prompt_str = prompt_template.format_prompt(schema=ResumeSchema.schema_json(), resume_text=raw_text[:6000]).to_string()\n",
        "\n",
        "    # Ask the LLM to return structured JSON\n",
        "    resp = llm.generate(messages=[{\"role\":\"user\", \"content\": prompt_str}])\n",
        "    # The 'resp' object structure may vary with langchain versions; fallback to text extraction:\n",
        "    # Try to fetch text from resp\n",
        "    try:\n",
        "        content = resp.generations[0][0].text\n",
        "    except Exception:\n",
        "        # fallback: get text in a simpler way\n",
        "        content = resp.generations[0][0].text if hasattr(resp, \"generations\") else str(resp)\n",
        "\n",
        "    # Parse the returned JSON using the pydantic parser - the parser enforces schema\n",
        "    parsed = parser.parse_raw(content)\n",
        "    return parsed.dict()\n"
      ],
      "metadata": {
        "id": "T7bwM_f4Pcsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage"
      ],
      "metadata": {
        "id": "rfwze8b2Pp7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import json\n",
        "    FILE = \"sample_resume.pdf\"\n",
        "    result = parse_resume_to_json(FILE, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    print(json.dumps(result, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "dHlOmswuPquP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}